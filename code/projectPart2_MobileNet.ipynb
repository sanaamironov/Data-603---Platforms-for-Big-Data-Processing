{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_db6bjbmmmt"
   },
   "source": [
    "# MobileNet With Pandas Python UDFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFMrg-1Ymmmu"
   },
   "source": [
    "# Launch Spark\n",
    "\n",
    "Three configuration items have to be added to the Spark configuration to enable Arrow as it is disabled by default. This can be done without modifying SparkLauncher now, but you can just modify that if you like.\n",
    "\n",
    "```python\n",
    "    # Apache Arrow Config\n",
    "    conf.set('spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT', '1')\n",
    "    conf.set('spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT', '1')\n",
    "    conf.set('spark.sql.execution.arrow.enabled', 'true')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaBofHJ_mmmz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import venv_pack\n",
    "\n",
    "def get_spark_conf():\n",
    "    \n",
    "    # Create Spark Configuration\n",
    "    print(f\"Creating Spark Configuration\")\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('yarn')\n",
    "\n",
    "    # Application Master Environment Variables -- ugly\n",
    "    conf.set('spark.yarn.appMasterEnv.JAVA_HOME', '/usr/java/jdk1.8.0_181-cloudera')\n",
    "    conf.set('spark.yarn.appMasterEnv.LD_LIBRARY_PATH', \n",
    "             '/opt/cloudera/parcels/CDH/lib64:/usr/java/jdk1.8.0_181-cloudera/jre/lib/amd64:/usr/java/jdk1.8.0_181-cloudera/jre/lib/amd64/server')\n",
    "\n",
    "    # Executor Envrironment Variables -- ugly\n",
    "    conf.set('spark.executorEnv.JAVA_HOME', '/usr/java/jdk1.8.0_181-cloudera')\n",
    "    conf.set('spark.executorEnv.LD_LIBRARY_PATH', \n",
    "             '/opt/cloudera/parcels/CDH/lib64:/usr/java/jdk1.8.0_181-cloudera/jre/lib/amd64:/usr/java/jdk1.8.0_181-cloudera/jre/lib/amd64/server')\n",
    "    conf.set('spark.executorEnv.HADOOP_HOME', \"/opt/cloudera/parcels/CDH\")\n",
    "    conf.set('spark.executorEnv.ARROW_LIBHDFS_DIR', \"/opt/cloudera/parcels/CDH/lib64\")\n",
    "    conf.set('spark.executorEnv.HADOOP_CONF_DIR', \"/etc/hadoop/conf\")\n",
    "    \n",
    "    app_name = f'{os.environ[\"USER\"]}_data603_spark'\n",
    "    conf.setAppName(app_name)\n",
    "    conf.set('spark.yarn.dist.archives', f'{os.environ[\"USER\"]}.tar.gz#{os.environ[\"USER\"]}')\n",
    "    conf.set('spark.pyspark.driver.python', f'\"source {os.environ[\"USER\"]}/bin/activate && {os.environ[\"USER\"]}/bin/python3\"')\n",
    "    conf.set('spark.yarn.appMasterEnv.PYSPARK_PYTHON', f'\"source {os.environ[\"USER\"]}/bin/activate && {os.environ[\"USER\"]}/bin/python3\"')\n",
    "    conf.set('spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON', f'\"source {os.environ[\"USER\"]}/bin/activate && {os.environ[\"USER\"]}/bin/python3\"')\n",
    "\n",
    "    conf.set('spark.yarn.appMasterEnv.HIVE_CONF_DIR', '/etc/hive/conf')\n",
    "    \n",
    "    #conf.set('spark.dynamicAllocation.minExecutors', '0')\n",
    "    conf.set('spark.dynamicAllocation.maxExecutors', '30')\n",
    "    \n",
    "    conf.set('spark.executor.cores', '30')\n",
    "    conf.set('spark.executor.memory', '60g')\n",
    "    conf.set('spark.executor.memoryOverhead', '10g')\n",
    "    conf.set('spark.yarn.am.memoryOverhead', '6g')\n",
    "    conf.set('spark.yarn.am.memory', '8g')\n",
    "    \n",
    "    conf.set('spark.driver.log.dfsDir', f'/user/spark/driverLogs')\n",
    "    conf.set('yarn.nodemanager.vmem-check-enabled',False)\n",
    "    \n",
    "    conf.set('spark.driver.extraJavaOptions', '-XX:ReservedCodeCacheSize=256M -XX:MaxMetaspaceSize=512m -XX:CompressedClassSpaceSize=512m')\n",
    "    conf.set('spark.executor.extraJavaOptions', '-XX:ReservedCodeCacheSize=256M -XX:MaxMetaspaceSize=512m -XX:CompressedClassSpaceSize=512m')\n",
    "    \n",
    "    conf.set('spark.driver.extraClassPath', '/etc/hadoop/conf:/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars')\n",
    "    conf.set('spark.executor.extraClassPath', '/etc/hadoop/conf:/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars')\n",
    "    #CDH-6.2.0-1.cdh6.2.0.p0.967373\n",
    "    conf.set('spark.port.maxRetries', 100)\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mj0n2AdQmmm1",
    "outputId": "bbeb63d5-63dc-4177-ade3-6a7d4adb3bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark Configuration\n"
     ]
    }
   ],
   "source": [
    "def get_spark_session(pack_venv = True, conf = get_spark_conf()):\n",
    "    \"\"\"\n",
    "    Launches Spark Context using UMBC Big Data Cluster YARN and returns a Spark Session.\n",
    "    \"\"\"\n",
    "    # Pack Virtual Environment\n",
    "    if pack_venv:\n",
    "        packed_environment_file = f\"{os.environ['USER']}.tar.gz\"\n",
    "        print(f\"Packing Virtual Environment: {packed_environment_file}\")\n",
    "        venv_pack.pack(output=packed_environment_file, force = True)\n",
    "    \n",
    "    # Set local environment variables\n",
    "    # for people that just won't follow directions and setup BASH\n",
    "    os.environ['JAVA_HOME'] = \"/usr/java/jdk1.8.0_181-cloudera\"\n",
    "    os.environ['CLASSPATH'] = \"/etc/hadoop/conf:/opt/cloudera/parcels/CDH/jars\"\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']}:{os.environ['JAVA_HOME']}/bin\"\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"/opt/cloudera/parcels/CDH/lib64\"\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{os.environ['LD_LIBRARY_PATH']}:{os.environ['JAVA_HOME']}/jre/lib/amd64\"\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{os.environ['LD_LIBRARY_PATH']}:{os.environ['JAVA_HOME']}/jre/lib/amd64/server\"\n",
    "\n",
    "    print(f\"Setting Environment Variables\")\n",
    "    os.environ['HADOOP_HOME'] = f\"/opt/cloudera/parcels/CDH\"\n",
    "    os.environ['SPARK_HOME'] = \"/opt/cloudera/parcels/CDH/lib/spark\"\n",
    "    os.environ['HIVE_HOME'] = \"/opt/cloudera/parcels/CDH/lib/hive\"\n",
    "    \n",
    "    os.environ['HADOOP_CONF_DIR'] = \"/etc/hadoop/conf\"\n",
    "    os.environ['YARN_CONF_DIR'] = \"/etc/hadoop/conf\"\n",
    "    os.environ['SPARK_CONF_DIR'] = \"/etc/spark/conf\"\n",
    "    os.environ['HIVE_CONF_DIR'] = \"/etc/hive/conf\"\n",
    "     \n",
    "    os.environ['PYSPARK_PYTHON'] = f'{os.environ[\"USER\"]}/bin/python3'\n",
    "    \n",
    "\n",
    "    # Create SparkSession\n",
    "    session_name = f\"{os.environ['USER']}_data603_spark_session\"\n",
    "    print(f\"Creating Spark Session: {session_name}\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf = conf)\\\n",
    "        .appName(session_name)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCtB1GsDmmm5",
    "outputId": "c675a14c-62fb-402f-ee0e-8f6bb72781f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /scratch/data603/a280/data603/SparkLauncher.ipynb\n",
      "Creating Spark Configuration\n",
      "Creating Spark Configuration\n",
      "Setting Environment Variables\n",
      "Creating Spark Session: a280_data603_spark_session\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from data603 import SparkLauncher\n",
    "\n",
    "# get a configuration object\n",
    "conf = SparkLauncher.get_spark_conf()\n",
    "\n",
    "# add a file to the configuration that will get copied to all the nodes on the cluster\n",
    "conf.set('spark.yarn.dist.files', 'keras_data/mobilenet_1_0_224_tf.h5')\n",
    "\n",
    "\n",
    "# launch the cluster using the configuration\n",
    "spark = SparkLauncher.get_spark_session(pack_venv = False, conf = conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T286S8Utmmm7"
   },
   "source": [
    "# Read Dataframe\n",
    "\n",
    "This must be done _BEFORE_ the UDF is defined because the UDF needs the schemas of the dataframes it will be using. I have a parquet file written out with the bounding boxes extracted of several bird types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OkuPtOe4mmm8"
   },
   "outputs": [],
   "source": [
    "image_chips = spark.read.parquet(\"/user/has1/chips_image.parquet\")\n",
    "image_chips = image_chips.drop('data') # remove the full-image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XHuT22mmmm_"
   },
   "outputs": [],
   "source": [
    "#Drop useless columns \n",
    "image_chips=image_chips.drop('hdfs_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXH_V2eXmmnC",
    "outputId": "49eedc32-a81b-4643-edc4-9e3bd6b5f395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------------+--------------------+\n",
      "|         ImageID|LabelName|      LabelText|           chip_data|\n",
      "+----------------+---------+---------------+--------------------+\n",
      "|025d25975e4275a2| /m/0c29q|        Leopard|[FF D8 FF E0 00 1...|\n",
      "|025d25975e4275a2| /m/0cd4d|        Cheetah|[FF D8 FF E0 00 1...|\n",
      "|025d25975e4275a2| /m/0449p|Jaguar (Animal)|[FF D8 FF E0 00 1...|\n",
      "|03bacd7be83b721e| /m/096mb|           Lion|[FF D8 FF E0 00 1...|\n",
      "|078bfcf1afb210ae| /m/096mb|           Lion|[FF D8 FF E0 00 1...|\n",
      "|078bfcf1afb210ae| /m/096mb|           Lion|[FF D8 FF E0 00 1...|\n",
      "|078bfcf1afb210ae| /m/096mb|           Lion|[FF D8 FF E0 00 1...|\n",
      "|0c9f40ea3014c553| /m/096mb|           Lion|[FF D8 FF E0 00 1...|\n",
      "|0e0e38e4ffb1b727| /m/0c29q|        Leopard|[FF D8 FF E0 00 1...|\n",
      "|0e0e38e4ffb1b727| /m/0cd4d|        Cheetah|[FF D8 FF E0 00 1...|\n",
      "|1465d9f311a8ef8c| /m/0c29q|        Leopard|[FF D8 FF E0 00 1...|\n",
      "|192a3715b2f2c738| /m/04g2r|           Lynx|[FF D8 FF E0 00 1...|\n",
      "|193d7fdc246af034| /m/07dm6|          Tiger|[FF D8 FF E0 00 1...|\n",
      "|235056bb502e8421| /m/0cd4d|        Cheetah|[FF D8 FF E0 00 1...|\n",
      "|350dd2f070b6f1be| /m/0cd4d|        Cheetah|[FF D8 FF E0 00 1...|\n",
      "|35ab5252f287cc08| /m/0c29q|        Leopard|[FF D8 FF E0 00 1...|\n",
      "|35ab5252f287cc08| /m/0cd4d|        Cheetah|[FF D8 FF E0 00 1...|\n",
      "|35ab5252f287cc08| /m/0cd4d|        Cheetah|[FF D8 FF E0 00 1...|\n",
      "|35ab5252f287cc08| /m/0449p|Jaguar (Animal)|[FF D8 FF E0 00 1...|\n",
      "|4ac1a8894dfcfffe| /m/07dm6|          Tiger|[FF D8 FF E0 00 1...|\n",
      "+----------------+---------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking to see its there\n",
    "image_chips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6W1lIgMlmmnF",
    "outputId": "edd39fb2-170b-46b8-fb99-14fc466565f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, LabelName: string, LabelText: string, chip_data: binary]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What are the datatypes\n",
    "image_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYVzUC4OmmnH"
   },
   "outputs": [],
   "source": [
    "def evaluate_chip(chip_data):\n",
    "    import io\n",
    "    import os\n",
    "    \n",
    "    from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "    from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "    from tensorflow.keras.applications.mobilenet import decode_predictions\n",
    "    from tensorflow.keras.preprocessing.image import load_img\n",
    "    from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "    # Load Model Data\n",
    "    model = MobileNet(weights = 'imagenet',\n",
    "                 include_top = True,alpha=1.0)\n",
    "\n",
    "    # Load the image\n",
    "    img = load_img(io.BytesIO(chip_data), target_size = (224,224))\n",
    "\n",
    "    # Prepare Image\n",
    "    image = img_to_array(img)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    \n",
    "    # Run prediction\n",
    "    yhat = model.predict(image)\n",
    "\n",
    "    # Decode Predictions\n",
    "    label = decode_predictions(yhat)\n",
    "    label = label[0][0]\n",
    "\n",
    "    ret = [label[1], float(label[2])]   \n",
    "\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrQZ7LBhmmnJ"
   },
   "outputs": [],
   "source": [
    "# make a UDF\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = ArrayType(StringType())\n",
    "\n",
    "udf_evaluate_chip = udf(evaluate_chip, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ls9FEGTSmmnM"
   },
   "outputs": [],
   "source": [
    "# evaluate image chips\n",
    "image_chips = image_chips.withColumn(\"prediction\", udf_evaluate_chip(\"chip_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Af9PqRG0mmnS",
    "outputId": "ca165ae1-1690-48cb-8292-4adcbcd632d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          prediction|\n",
      "+--------------------+\n",
      "|[jaguar, 0.946791...|\n",
      "|[jaguar, 0.981539...|\n",
      "|[jaguar, 0.959353...|\n",
      "|[cougar, 0.984374...|\n",
      "|[skunk, 0.2050361...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show what the model predicted\n",
    "image_chips.select('prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HID69aBDmmnW",
    "outputId": "9969f56f-20ab-49ce-bfeb-99a868f2dec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, LabelName: string, LabelText: string, chip_data: binary, prediction: array<string>]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDIQXHXkmmnY"
   },
   "outputs": [],
   "source": [
    "image_chips1 = image_chips.select('ImageID','LabelText',image_chips.prediction[0],image_chips.prediction[1])\n",
    "\n",
    "image_chips1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "projectPart2 (1)(2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
