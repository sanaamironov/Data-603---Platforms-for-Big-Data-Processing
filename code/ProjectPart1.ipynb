{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# PreProcessing Google Open Image Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to Spark and HDFS\n",
    "def get_hdfs(namenode = '10.3.0.2', port = 8020):\n",
    "    \"\"\"\n",
    "    Return a HDFS connection. Note: this API is deprecated in pyarrow\n",
    "    \"\"\"\n",
    "    import pyarrow as pa\n",
    "    import os\n",
    "\n",
    "    os.environ['HADOOP_HOME'] = f\"/opt/cloudera/parcels/CDH\"\n",
    "    os.environ['JAVA_HOME'] = \"/usr/java/jdk1.8.0_181-cloudera\"\n",
    "    os.environ['ARROW_LIBHDFS_DIR'] = \"/opt/cloudera/parcels/CDH/lib64\"\n",
    "               \n",
    "    hdfs = pa.hdfs.HadoopFileSystem(namenode, port)\n",
    "    hdfs.connect()\n",
    "    \n",
    "    return hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_httpdfs():\n",
    "    import os\n",
    "    from hdfs import InsecureClient\n",
    "    client = InsecureClient('http://10.3.0.2:9870', user=os.environ['USER'])\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs(namenode = '10.3.0.2', port = 8020):\n",
    "    \"\"\"\n",
    "    Returns generic Pyarrow filesystem object connected to HDFS.\n",
    "    \n",
    "    example usage: hdfs.get_target_stats(fs.FileSelector('/data/google_open_image', recursive = True))\n",
    "    \n",
    "    This is the hdfs interface going forward as the other is deprecated.\n",
    "    \"\"\"\n",
    "    from pyarrow import fs\n",
    "    \n",
    "    hdfs_options = fs.HdfsOptions(endpoint = (namenode, port), driver = 'libhdfs')\n",
    "    hdfs = fs.HadoopFileSystem(hdfs_options)\n",
    "    \n",
    "    return hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /scratch/data603/has1/data603/HDFS.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['densenet',\n",
       " 'efficientnet',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3',\n",
       " 'nasnet',\n",
       " 'resnet',\n",
       " 'vgg16',\n",
       " 'vgg19',\n",
       " 'xception']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from data603 import HDFS\n",
    "\n",
    "httpdfs = get_httpdfs()\n",
    "httpdfs.list('/data/keras_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet101_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnet101v2_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnet152_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnet152v2_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnet50v2_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnext101_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnext101_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
       " 'resnext50_weights_tf_dim_ordering_tf_kernels.h5',\n",
       " 'resnext50_weights_tf_dim_ordering_tf_kernels_notop.h5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "httpdfs.list('/data/keras_models/resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "# Create a local directory\n",
    "import os\n",
    "keras_data = './keras_data'\n",
    "if(not os.path.exists(keras_data)):\n",
    "    os.mkdir(keras_data)\n",
    "\n",
    "#download file from hdfs\n",
    " \n",
    "url = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "req = urlopen(\"http://www.google.com/\").read()\n",
    "\n",
    "\n",
    "with open('./keras_data/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', 'wb') as f:\n",
    "    f.write(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /scratch/data603/has1/data603/SparkLauncher.ipynb\n",
      "Creating Spark Configuration\n",
      "Creating Spark Configuration\n",
      "Packing Virtual Environment: has1.tar.gz\n",
      "Setting Environment Variables\n",
      "Creating Spark Session: has1_data603_spark_session\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from data603 import SparkLauncher\n",
    "\n",
    "# get a configuration object\n",
    "conf = SparkLauncher.get_spark_conf()\n",
    "\n",
    "# add a file to the configuration that will get copied to all the nodes on the cluster\n",
    "conf.set('spark.yarn.dist.files', './keras_data/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "conf.set('spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT', '1')\n",
    "conf.set('spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT', '1')\n",
    "conf.set('spark.sql.execution.arrow.enabled', 'true')\n",
    "conf.set('spark.dynamicAllocation.minExecutors', '1')\n",
    "\n",
    "conf.set('spark.dynamicAllocation.maxExecutors', '7')\n",
    "\n",
    "conf.set('spark.executor.cores', '16')\n",
    "\n",
    "conf.set('spark.executor.memory', '50g')\n",
    "#conf.set('spark.sql.autoBroadcastJoinThreshold', '-1')\n",
    "\n",
    "# launch the cluster using the configuration\n",
    "spark = SparkLauncher.get_spark_session(pack_venv = True, conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import *\n",
    "#Reading image label\n",
    "labels = spark.read.csv('/data/google_open_image/metadata/class-descriptions-boxable.csv', \n",
    "                        header = False,\n",
    "                        schema = StructType([StructField(\"LabelName\", StringType()), \n",
    "                                             StructField(\"LabelText\", StringType())]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|LabelName|      LabelText|\n",
      "+---------+---------------+\n",
      "| /m/0449p|Jaguar (Animal)|\n",
      "| /m/04g2r|           Lynx|\n",
      "| /m/07dm6|          Tiger|\n",
      "| /m/096mb|           Lion|\n",
      "| /m/0c29q|        Leopard|\n",
      "| /m/0cd4d|        Cheetah|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter out to only big cats within the labels \n",
    "labels = labels.filter(\"LabelText = 'Tiger' OR LabelText = 'Lion' OR LabelText = 'Cheetah' OR LabelText = 'Leopard' OR LabelText = 'Jaguar (Animal)' OR LabelText = 'Lynx' \")\n",
    "\n",
    "labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for the data so the Confidence is a number, not a string\n",
    "label_schema = StructType([\n",
    "    StructField(\"ImageID\", StringType()),\n",
    "    StructField(\"Source\", StringType()),\n",
    "    StructField(\"LabelName\", StringType()),\n",
    "    StructField(\"Confidence\", DoubleType())\n",
    "])\n",
    "\n",
    "# Read in the csv files using the schema\n",
    "image_labels_1 = spark.read\\\n",
    "                    .csv('/data/google_open_image/labels/test-annotations-human-imagelabels-boxable.csv', \n",
    "                        header = True,\n",
    "                        schema = label_schema)\n",
    "image_labels_2 = spark.read\\\n",
    "                    .csv('/data/google_open_image/labels/train-annotations-human-imagelabels-boxable.csv', \n",
    "                        header = True,\n",
    "                        schema = label_schema)\n",
    "image_labels_3 = spark.read\\\n",
    "                    .csv('/data/google_open_image/labels/validation-annotations-human-imagelabels-boxable.csv', \n",
    "                        header = True,\n",
    "                        schema = label_schema)\n",
    "\n",
    "# join the 3 files into one large dataframe\n",
    "image_labels = image_labels_1.union(image_labels_2).union(image_labels_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join and filter\n",
    "image_labels = image_labels.join(labels, on = 'LabelName', how = 'right')\\\n",
    "                .filter(\"Confidence > 0.99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct image IDs.\n",
    "image_ids = image_labels.filter(\"Confidence > 0.99\").select('ImageID').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3939"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many images there are.\n",
    "image_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the raw image data\n",
    "images_parquet = spark.read.parquet('/etl/google_open_image/images_coalesced.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a lot of columns that aren't needed, select just the ones of interest.\n",
    "images_parquet = images_parquet.select(['ImageID', 'Subset', 'Data'])\\\n",
    "                .withColumn(\"ImageID\", F.lower(F.col('ImageID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, Subset: string, Data: binary]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the column names in the dataframe\n",
    "images_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining image ids to the parquet images to filter out things we dont want\n",
    "images_parquet = image_ids.join(images_parquet, on = 'ImageID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the dataframe\n",
    "images_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 3 bounding box csv files.\n",
    "bounding_boxes_1 = spark.read.csv('/data/google_open_image/bboxes/test-annotations-bbox.csv', header = True)\n",
    "bounding_boxes_2 = spark.read.csv('/data/google_open_image/bboxes/train-annotations-bbox.csv', header = True)\n",
    "bounding_boxes_3 = spark.read.csv('/data/google_open_image/bboxes/validation-annotations-bbox.csv', header = True)\n",
    "\n",
    "# Join the dataframes into a single dataframe.\n",
    "bounding_boxes = bounding_boxes_1.union(bounding_boxes_2).union(bounding_boxes_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on ImageID to get just the bounding boxes we have image data for.\n",
    "bbs = image_ids.join(bounding_boxes, on = 'ImageID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join in the labels so there are human-readable labels on the bounding boxes.\n",
    "bbs = labels.join(bbs, on = 'LabelName', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many boxes there are.\n",
    "bbs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_chips = images_parquet.join(bbs, on = 'ImageID', how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many chips we have to make sure the join was the correct one.\n",
    "image_chips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chip(data, xmin, xmax, ymin, ymax):\n",
    "    from PIL import Image\n",
    "    import io, math\n",
    "    \n",
    "    # Read the image data using Pillow\n",
    "    img = Image.open(io.BytesIO(data))\n",
    "    # Get the size of the image \n",
    "    (width, height) = img.size\n",
    "    \n",
    "    # Calculate the bounding box pixels\n",
    "    # observe the use of float function here. That's necessary\n",
    "    # because the bounding box data were read in as strings, not doubles.\n",
    "    left = math.floor(float(xmin)*width)\n",
    "    upper = math.floor(float(ymin)*height)\n",
    "    right = math.floor(float(xmax)*width)\n",
    "    lower = math.floor(float(ymax)*height)\n",
    "    \n",
    "    # Crop the image to the bounding box size\n",
    "    img = img.crop(box = (left, upper, right, lower))\n",
    "    \n",
    "    # Save the image to a byte-buffer\n",
    "    buff = io.BytesIO()\n",
    "    img.save(buff, format = \"JPEG\")\n",
    "    \n",
    "    # Get the raw bytes of the jpeg data.\n",
    "    byte_array = buff.getvalue()\n",
    "    return byte_array   # return buff.getvalue() doesn't work. This a quirk of pyspark not being able to determine the output type of a function call.\n",
    "\n",
    "# Wrap the function as a spark udf (user-defined function) with a binary return type\n",
    "udf_extract_chip = F.udf(extract_chip, returnType = BinaryType())\n",
    "\n",
    "# Create a new column with the image chip data\n",
    "image_chips = image_chips.withColumn(\"chip_data\", udf_extract_chip(\"Data\",\"XMin\",\"XMax\",\"YMin\",\"YMax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, Subset: string, Data: binary, LabelName: string, LabelText: string, Source: string, Confidence: string, XMin: string, XMax: string, YMin: string, YMax: string, IsOccluded: string, IsTruncated: string, IsGroupOf: string, IsDepiction: string, IsInside: string, chip_data: binary]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all useless feilds to save space \n",
    "image_chips=image_chips.drop('IsOccluded','IsGroupOf','Source','IsTruncated','IsDepiction','IsInside','Data','XMin','XMax','YMin','YMax','Subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, LabelName: string, LabelText: string, chip_data: binary]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+--------------------+\n",
      "|         ImageID|LabelName|LabelText|           chip_data|\n",
      "+----------------+---------+---------+--------------------+\n",
      "|09708a3c4d623287| /m/096mb|     Lion|[FF D8 FF E0 00 1...|\n",
      "|12b5606107c34487| /m/096mb|     Lion|[FF D8 FF E0 00 1...|\n",
      "|1321281d74240f59| /m/07dm6|    Tiger|[FF D8 FF E0 00 1...|\n",
      "|1321281d74240f59| /m/07dm6|    Tiger|[FF D8 FF E0 00 1...|\n",
      "|178f377c72af95ba| /m/04g2r|     Lynx|[FF D8 FF E0 00 1...|\n",
      "|2da1984dacb751c1| /m/07dm6|    Tiger|[FF D8 FF E0 00 1...|\n",
      "|3a831a9f7c0e5b36| /m/096mb|     Lion|[FF D8 FF E0 00 1...|\n",
      "|3f11b18d2f35089a| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "|3f11b18d2f35089a| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "|4ed7e504af1e8c92| /m/0c29q|  Leopard|[FF D8 FF E0 00 1...|\n",
      "|4ed7e504af1e8c92| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "|720d627bc86e7d74| /m/04g2r|     Lynx|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0c29q|  Leopard|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0c29q|  Leopard|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0c29q|  Leopard|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0c29q|  Leopard|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "|78f333185baec190| /m/0cd4d|  Cheetah|[FF D8 FF E0 00 1...|\n",
      "+----------------+---------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ran out of memory when running this. its fine\n",
    "image_chips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write chip to hdfs\n",
    "def write_chip_hdfs(data, id, label):\n",
    "    import io\n",
    "    from random import randint\n",
    "    \n",
    "    from hdfs import InsecureClient\n",
    "    client = InsecureClient('http://10.3.0.2:9870', user='has1')\n",
    "    \n",
    "    filename = f\"{label}_{id}_{randint(0,1000000)}.jpeg\"\n",
    "    path = \"/user/has1/write_chips_final/\" + filename\n",
    "    client.write(path, io.BytesIO(data))\n",
    "    \n",
    "    return path\n",
    "\n",
    "# wrap function in a udf\n",
    "udf_write_chip_hdfs = F.udf(write_chip_hdfs, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dont really need this\n",
    "image_chips = image_chips.withColumn(\"hdfs_path\", udf_write_chip_hdfs(\"chip_data\", \"ImageID\", \"LabelText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_chips.write.mode(\"overwrite\").parquet(\"/user/has1/chips_image.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
