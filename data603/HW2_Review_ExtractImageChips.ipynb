{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 2 Review\n",
    "\n",
    "I am among the sad souls that had done the homework, but lost everything during the cluster login node crash. But since I'm the instructor, I am allowed to not re-do the homework. Instead I decided to put this notebook together for everyone. It has some similarities to the homework (it extracts sub-images/image chips) using bounding box data. Instead of reading and writing dataframes to files I added a section of how to run the image chips through a Keras model on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rework of data603 library .ipynb files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDFS.ipynb\n",
    "\n",
    "I updated this notebook with another way to use HDFS. While doing this exercise I was unable to get `pyarrow` configured correctly on the worker nodes, _however_ I was able to use the HDFS HTTP interface to connect. See the notebook for details and a link to the API. The other methods are still there, but if you want a udf to connect to HDFS, you'll need to borrow code from the HDFS.ipynb. Here I use this new interface to list files in HDFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /scratch/data603/klucar/data603/HDFS.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['densenet',\n",
       " 'efficientnet',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3',\n",
       " 'nasnet',\n",
       " 'resnet',\n",
       " 'vgg16',\n",
       " 'vgg19',\n",
       " 'xception']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from data603 import HDFS\n",
    "\n",
    "httpdfs = HDFS.get_httpdfs()\n",
    "httpdfs.list('/data/keras_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras Weight Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I took the opportunity to download all the built-in Keras models weight files. The details of the file naming convention are beyond this demonstration, but reach out and I can help you, or read the Keras code because that's what I did! The directory names are the names of the various models built in to Keras. Each directory contains several .h5 weight files for each model. The models have differrent parameters such as input image size, that vary with each model. For this demonstration I'll use the `mobilenet` model because it is optimized to run on mobile hardware, so it should run faster than other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobilenet_1_0_128_tf.h5',\n",
       " 'mobilenet_1_0_128_tf_no_top.h5',\n",
       " 'mobilenet_1_0_160_tf.h5',\n",
       " 'mobilenet_1_0_160_tf_no_top.h5',\n",
       " 'mobilenet_1_0_192_tf.h5',\n",
       " 'mobilenet_1_0_192_tf_no_top.h5',\n",
       " 'mobilenet_1_0_224_tf.h5',\n",
       " 'mobilenet_1_0_224_tf_no_top.h5',\n",
       " 'mobilenet_2_5_128_tf.h5',\n",
       " 'mobilenet_2_5_128_tf_no_top.h5',\n",
       " 'mobilenet_2_5_160_tf.h5',\n",
       " 'mobilenet_2_5_160_tf_no_top.h5',\n",
       " 'mobilenet_2_5_192_tf.h5',\n",
       " 'mobilenet_2_5_192_tf_no_top.h5',\n",
       " 'mobilenet_2_5_224_tf.h5',\n",
       " 'mobilenet_2_5_224_tf_no_top.h5',\n",
       " 'mobilenet_5_0_128_tf.h5',\n",
       " 'mobilenet_5_0_128_tf_no_top.h5',\n",
       " 'mobilenet_5_0_160_tf.h5',\n",
       " 'mobilenet_5_0_160_tf_no_top.h5',\n",
       " 'mobilenet_5_0_192_tf.h5',\n",
       " 'mobilenet_5_0_192_tf_no_top.h5',\n",
       " 'mobilenet_5_0_224_tf.h5',\n",
       " 'mobilenet_5_0_224_tf_no_top.h5',\n",
       " 'mobilenet_7_5_128_tf.h5',\n",
       " 'mobilenet_7_5_128_tf_no_top.h5',\n",
       " 'mobilenet_7_5_160_tf.h5',\n",
       " 'mobilenet_7_5_160_tf_no_top.h5',\n",
       " 'mobilenet_7_5_192_tf.h5',\n",
       " 'mobilenet_7_5_192_tf_no_top.h5',\n",
       " 'mobilenet_7_5_224_tf.h5',\n",
       " 'mobilenet_7_5_224_tf_no_top.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "httpdfs.list('/data/keras_models/mobilenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The file I need is the `mobilenet_1_0_224_tf.h5` This is the weight file for 224x224 pixel input images and alpha of 1.0, meaning use full-size filters in the hidden layers. The _no_top_ files don't have the final output layer and are meant to be used to re-train the models with your own labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download Weight File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobilenet_1_0_224_tf.h5',\n",
       " 'imagenet_class_index.json',\n",
       " 'bbox_labels_600_hierarchy.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'Ostrich_f20950a7e7db32d4_344161.jpeg',\n",
       " 'Magpie_387d01e608451323_784664.jpeg',\n",
       " 'Goose_f7fe3c8c98e5770d_190299.jpeg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a local directory\n",
    "import os\n",
    "keras_data = './keras_data'\n",
    "if(not os.path.exists(keras_data)):\n",
    "    os.mkdir(keras_data)\n",
    "\n",
    "# download file from hdfs\n",
    "mobilenet_weight_file = 'mobilenet_1_0_224_tf.h5'\n",
    "local_weight_file = f\"{keras_data}/{mobilenet_weight_file}\"\n",
    "if(not os.path.exists(local_weight_file)):\n",
    "    httpdfs.download(f\"/data/keras_models/mobilenet/{mobilenet_weight_file}\", local_weight_file)\n",
    "\n",
    "# check local file exists\n",
    "os.listdir(keras_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkLauncher.ipynb\n",
    "\n",
    "Instead of constantly handing out new versions of this notebook (too late, I know) I decided to break the function into two functions. The first function returns a configuration object and the second takes this configuration as input to create a spark.sql session. This allows you the opportunity to add your own configuration items before launching Spark, but it is backward compatible with the older versions so your old notebooks should still work without any code updates. I use this to distribute the .h5 weight file to each spark node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /scratch/data603/klucar/data603/SparkLauncher.ipynb\n",
      "Creating Spark Configuration\n",
      "Creating Spark Configuration\n",
      "Setting Environment Variables\n",
      "Creating Spark Session: klucar_data603_spark_session\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from data603 import SparkLauncher\n",
    "\n",
    "# get a configuration object\n",
    "conf = SparkLauncher.get_spark_conf()\n",
    "\n",
    "# add a file to the configuration that will get copied to all the nodes on the cluster\n",
    "conf.set('spark.yarn.dist.files', './keras_data/mobilenet_1_0_224_tf.h5')\n",
    "\n",
    "# launch the cluster using the configuration\n",
    "spark = SparkLauncher.get_spark_session(pack_venv = False, conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extracting Image Chips\n",
    "\n",
    "Now that Spark is running and it has our model weight file, we need to extract sub-images from the google open image dataset. This is the part that is very similar to homework 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Find Some Interesting Chips\n",
    "\n",
    "Most of the Keras Models use ResNet image labels. ResNet has 1000 labels. Here are some labels of birds that are common between the two datasets:\n",
    "\n",
    "|Google OID | ResNet |\n",
    "|-----------|--------|\n",
    "| Magpie | magpie |\n",
    "| Ostrich | ostrich |\n",
    "| Goose | goose |\n",
    "\n",
    "I searched the OID labels by examining the OID label hierarchy [here.](https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy_visualizer/circle.html) It's also available for download in JSON format [here.](https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json)\n",
    "\n",
    "I downloaded the Resnet labels from [Kaggle](https://www.kaggle.com/alexisbcook/resnet50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load Class Descriptions Into Spark Dataframe\n",
    "\n",
    "Example File Format, no header\n",
    "\n",
    "```\n",
    "...\n",
    "/m/0pc9,Alphorn\n",
    "/m/0pckp,Robin\n",
    "/m/0pcm_,Larch\n",
    "/m/0pcq81q,Soccer player\n",
    "/m/0pcr,Alpaca\n",
    "/m/0pcvyk2,Nem\n",
    "/m/0pd7,Army\n",
    "/m/0pdnd2t,Bengal clockvine\n",
    "/m/0pdnpc9,Bushwacker\n",
    "/m/0pdnsdx,Enduro\n",
    "/m/0pdnymj,Gekkonidae\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "labels = spark.read.csv('/data/google_open_image/metadata/class-descriptions-boxable.csv', \n",
    "                        header = False,\n",
    "                        schema = StructType([StructField(\"LabelName\", StringType()), \n",
    "                                             StructField(\"LabelText\", StringType())]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Filter labels to just the three we're interested in _Magpie_ _Ostrich_ and _Goose_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LabelName</th>\n",
       "      <th>LabelText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/012074</td>\n",
       "      <td>Magpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/05n4y</td>\n",
       "      <td>Ostrich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/0dbvp</td>\n",
       "      <td>Goose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LabelName LabelText\n",
       "0  /m/012074    Magpie\n",
       "1   /m/05n4y   Ostrich\n",
       "2   /m/0dbvp     Goose"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels.filter(\"LabelText = 'Magpie' OR LabelText = 'Ostrich' OR LabelText = 'Goose'\")\n",
    "\n",
    "# since it's just 3 labels, bring it back as a pandas dataframe\n",
    "pd_labels = labels.toPandas()\n",
    "pd_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download Image IDs with their labels\n",
    "\n",
    "File Format Example\n",
    "```\n",
    "ImageID,Source,LabelName,Confidence\n",
    "000026e7ee790996,verification,/m/04hgtk,0\n",
    "000026e7ee790996,verification,/m/07j7r,1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a schema for the data so the Confidence is a number, not a string\n",
    "label_schema = StructType([\n",
    "    StructField(\"ImageID\", StringType()),\n",
    "    StructField(\"Source\", StringType()),\n",
    "    StructField(\"LabelName\", StringType()),\n",
    "    StructField(\"Confidence\", DoubleType())\n",
    "])\n",
    "\n",
    "# Read in the csv files using the schema\n",
    "image_labels_1 = spark.read\\\n",
    "                    .csv('/data/google_open_image/labels/test-annotations-human-imagelabels-boxable.csv', \n",
    "                        header = True,\n",
    "                        schema = label_schema)\n",
    "image_labels_2 = spark.read\\\n",
    "                    .csv('/data/google_open_image/labels/train-annotations-human-imagelabels-boxable.csv', \n",
    "                        header = True,\n",
    "                        schema = label_schema)\n",
    "image_labels_3 = spark.read\\\n",
    "                    .csv('/data/google_open_image/labels/validation-annotations-human-imagelabels-boxable.csv', \n",
    "                        header = True,\n",
    "                        schema = label_schema)\n",
    "\n",
    "# join the 3 files into one large dataframe\n",
    "image_labels = image_labels_1.union(image_labels_2).union(image_labels_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check the image labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>n_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verification</td>\n",
       "      <td>1906725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crowdsource-verification</td>\n",
       "      <td>276745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Source  n_images\n",
       "0              verification   1906725\n",
       "1  crowdsource-verification    276745"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the distinct number of image Ids. Should be 1.9 million\n",
    "# Every image can have more than one label, so there are many more\n",
    "# labels than image IDs.\n",
    "image_labels.groupBy('Source')\\\n",
    "    .agg(F.countDistinct(\"ImageId\").alias(\"n_images\"))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filter image labels to labels of interest\n",
    "\n",
    "Joining the `image_labels` with the `labels` dataframe adds the `LabelName` column to the larger `image_labels` dataframe.\n",
    "\n",
    "Then filter to only high-confidence labels and just grab the image IDs of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# join and filter\n",
    "image_labels = image_labels.join(labels, on = 'LabelName', how = 'right')\\\n",
    "                .filter(\"Confidence > 0.99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>n_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verification</td>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Source  n_images\n",
       "0  verification      3755"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the dataframe. Count how many images we have after filtering.\n",
    "image_labels.groupBy('Source')\\\n",
    "    .agg(F.countDistinct(\"ImageId\").alias(\"n_images\"))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get just the image IDs of interest.\n",
    "\n",
    "A dataframe of the distinct image IDs will be used to only get the needed images from the parquet file of all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# distinct image IDs.\n",
    "image_ids = image_labels.filter(\"Confidence > 0.99\").select('ImageID').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many images there are.\n",
    "image_ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read the Parquet file containing the dataframe with the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images_parquet = spark.read.parquet('/etl/google_open_image/images_coalesced.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# there's a lot of columns that aren't needed, select just the ones of interest.\n",
    "images_parquet = images_parquet.select(['ImageID', 'Subset', 'Data'])\\\n",
    "                .withColumn(\"ImageID\", F.lower(F.col('ImageID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, Subset: string, Data: binary]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the column names in the dataframe\n",
    "images_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filter images to just those of interest.\n",
    "\n",
    "This is where we use the `image_ids` dataframe to filter the parquet file dataframe to just the data needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images_parquet = image_ids.join(images_parquet, on = 'ImageID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the dataframe\n",
    "images_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bounding Boxes\n",
    "\n",
    "Read in all the bounding box data similarly to how the image labels were read in. Notice that a schema wasn't used, which actually comes to bite us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read the 3 bounding box csv files.\n",
    "bounding_boxes_1 = spark.read.csv('/data/google_open_image/bboxes/test-annotations-bbox.csv', header = True)\n",
    "bounding_boxes_2 = spark.read.csv('/data/google_open_image/bboxes/train-annotations-bbox.csv', header = True)\n",
    "bounding_boxes_3 = spark.read.csv('/data/google_open_image/bboxes/validation-annotations-bbox.csv', header = True)\n",
    "\n",
    "# Join the dataframes into a single dataframe.\n",
    "bounding_boxes = bounding_boxes_1.union(bounding_boxes_2).union(bounding_boxes_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, Source: string, LabelName: string, Confidence: string, XMin: string, XMax: string, YMin: string, YMax: string, IsOccluded: string, IsTruncated: string, IsGroupOf: string, IsDepiction: string, IsInside: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the schema\n",
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Again, use the images_ids dataframe to filter to just the bounding boxes required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Join on ImageID to get just the bounding boxes we have image data for.\n",
    "bbs = image_ids.join(bounding_boxes, on = 'ImageID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Join in the labels so there are human-readable labels on the bounding boxes.\n",
    "bbs = labels.join(bbs, on = 'LabelName', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10176"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many boxes there are.\n",
    "bbs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finally, we're ready to extract image chips from the images\n",
    "\n",
    "This join actually makes copies of the image data for each bounding box. While this isn't ideal, it makes extracting the image chips defined by the bounding boxes easier to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_chips = images_parquet.join(bbs, on = 'ImageID', how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10176"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many chips we have to make sure the join was the correct one.\n",
    "image_chips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ImageID: string, Subset: string, Data: binary, LabelName: string, LabelText: string, Source: string, Confidence: string, XMin: string, XMax: string, YMin: string, YMax: string, IsOccluded: string, IsTruncated: string, IsGroupOf: string, IsDepiction: string, IsInside: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the Schema\n",
    "image_chips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Cache the dataframe\n",
    "\n",
    "Now that we have a dataframe of an appropriate size, cache it so future operations are faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#image_chips.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remember that the `cache()` operation isn't evaluated right away. The next operation on the dataframe will take longer because it must calculate the dataframe and store it in memory. After that, things shoud speed up. Let's test that theory out with a calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Run the same calculation twice and compare the run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#\n",
    "#begin = time.perf_counter()\n",
    "#image_chips.groupby('LabelText').agg(F.countDistinct('ImageID').alias('n_images')).toPandas()\n",
    "#end = time.perf_counter()\n",
    "#\n",
    "#print(f\"First evaluation took {end - begin} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#\n",
    "#begin = time.perf_counter()\n",
    "#image_chips.groupby('LabelText').agg(F.countDistinct('ImageID').alias('n_images')).toPandas()\n",
    "#end = time.perf_counter()\n",
    "#\n",
    "#print(f\"Second evaluation took {end - begin} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As you can see the second run was significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define a UDF to extract the portion of the image defined by the bounding box.\n",
    "\n",
    "The bounding box coordinates are given as fractions of the number of pixels in the image. For example: XMin = 0.25 if the image has a width (X) of 1000 pixels, XMin is pixel 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def extract_chip(data, xmin, xmax, ymin, ymax):\n",
    "    from PIL import Image\n",
    "    import io, math\n",
    "    \n",
    "    # Read the image data using Pillow\n",
    "    img = Image.open(io.BytesIO(data))\n",
    "    # Get the size of the image \n",
    "    (width, height) = img.size\n",
    "    \n",
    "    # Calculate the bounding box pixels\n",
    "    # observe the use of float function here. That's necessary\n",
    "    # because the bounding box data were read in as strings, not doubles.\n",
    "    left = math.floor(float(xmin)*width)\n",
    "    upper = math.floor(float(ymin)*height)\n",
    "    right = math.floor(float(xmax)*width)\n",
    "    lower = math.floor(float(ymax)*height)\n",
    "    \n",
    "    # Crop the image to the bounding box size\n",
    "    img = img.crop(box = (left, upper, right, lower))\n",
    "    \n",
    "    # Save the image to a byte-buffer\n",
    "    buff = io.BytesIO()\n",
    "    img.save(buff, format = \"JPEG\")\n",
    "    \n",
    "    # Get the raw bytes of the jpeg data.\n",
    "    byte_array = buff.getvalue()\n",
    "    return byte_array   # return buff.getvalue() doesn't work. This a quirk of pyspark not being able to determine the output type of a function call.\n",
    "\n",
    "# Wrap the function as a spark udf (user-defined function) with a binary return type\n",
    "udf_extract_chip = F.udf(extract_chip, returnType = BinaryType())\n",
    "\n",
    "# Create a new column with the image chip data\n",
    "image_chips = image_chips.withColumn(\"chip_data\", udf_extract_chip(\"Data\",\"XMin\",\"XMax\",\"YMin\",\"YMax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# cache the chips\n",
    "#image_chips.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Check the chips dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10176"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force the cache to happen\n",
    "image_chips.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Write Just the Chips to a Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://worker2.hdp-internal:8020/user/klucar/image_chips_no_data.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/scratch/data603_virtualenv/klucar/lib64/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/data603_virtualenv/klucar/lib64/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o760.parquet.\n: org.apache.spark.sql.AnalysisException: path hdfs://worker2.hdp-internal:8020/user/klucar/image_chips_no_data.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b9e111f48ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_chips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_chips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_chips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/klucar/image_chips_no_data.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/data603_virtualenv/klucar/lib64/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/data603_virtualenv/klucar/lib64/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/data603_virtualenv/klucar/lib64/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://worker2.hdp-internal:8020/user/klucar/image_chips_no_data.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "image_chips = image_chips.drop('data')\n",
    "image_chips.write.parquet(\"/user/klucar/image_chips_no_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OFF THE RAILS\n",
    "\n",
    "Some things got busy on the cluster! Here I wrote out the chip data to HDFS as files just to show how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To use the chips later, write them to HDFS\n",
    "\n",
    "Here I use a different hdfs library. This is because I was unable to correctly configure `pyarrow` to work on the cluster nodes. See the new HDFS.ipynb file for details on this.\n",
    "\n",
    "Instead of pulling every image chip back to the notebook and then uploading it to HDFS, create a UDF that writes a chip file to HDFS and evaluate it on the dataframe. This will write the chips from every spark node _in parallel_ across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# write chip to hdfs\n",
    "def write_chip_hdfs(data, id, label):\n",
    "    import io\n",
    "    from random import randint\n",
    "    \n",
    "    from hdfs import InsecureClient\n",
    "    client = InsecureClient('http://10.3.0.2:9870', user='klucar')\n",
    "    \n",
    "    filename = f\"{label}_{id}_{randint(0,1000000)}.jpeg\"\n",
    "    path = \"/user/klucar/keras_chips/\" + filename\n",
    "    client.write(path, io.BytesIO(data))\n",
    "    \n",
    "    return path\n",
    "\n",
    "# wrap function in a udf\n",
    "udf_write_chip_hdfs = F.udf(write_chip_hdfs, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Call the UDF\n",
    "\n",
    "```python\n",
    "image_chips = image_chips.withColumn(\"hdfs_path\", udf_write_chip_hdfs(\"chip_data\", \"ImageID\", \"LabelText\"))\n",
    "image_chips.agg(F.countDistinct('hdfs_path')).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the chips and examine some outputs.\n",
    "#image_chips.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
